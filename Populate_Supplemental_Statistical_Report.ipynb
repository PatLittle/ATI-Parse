{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Populate Supplemental Statistical Report from consolidated CSV\n",
        "\n",
        "This notebook fills the Supplemental Statistical Report template from the consolidated Supplemental dataset.\n",
        "\n",
        "- Select institution, reporting period, and language.\n",
        "- Generates a new .xlsx file with the supplemental sheet populated.\n",
        "- Uses ATIP_ForConsumption mapping for cell targets.\n",
        "- Includes a mapping accuracy report based on dataset header similarity to spreadsheet labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install ipywidgets\n",
        "import re\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.utils import column_index_from_string, get_column_letter\n",
        "from openpyxl.worksheet.formula import ArrayFormula\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CSV_URL = None  # TODO: set to hosted URL (e.g., https://.../Supplemental-AI_refactored.csv)\n",
        "CSV_FALLBACK_PATH = \"Supplemental-AI_refactored.csv\"\n",
        "\n",
        "TEMPLATE_EN = \"Copy of 2024-2025 Supplemental Statistical Report.xlsx\"\n",
        "TEMPLATE_FR = \"2024-2025 Rapport statistique suppl\u00e9mentaire.xlsx\"\n",
        "MAPPING_SHEET = \"ATIP_ForConsumption\"\n",
        "SUPP_SHEET_EN = \"Supplemental Report 2024-25\"\n",
        "SUPP_SHEET_FR = \"Rapport suppl\u00e9mentaire 2024-25\"\n",
        "SUPP_DATE_START_CELL = \"D9\"\n",
        "SUPP_DATE_END_CELL = \"H9\"\n",
        "\n",
        "SCHEMA_PATH = \"combined_supplemental_form_data.xlsx\"\n",
        "DATASET_HEADER_URL = \"https://open.canada.ca/data/dataset/236294e1-bc74-486f-ab97-422227bc8832/resource/d812b0b9-320f-465c-8c20-66ef90b76db5/download/supplemental-dataset-2024-25.xlsx\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DIRECT_REF_RE = re.compile(r\"=\\s*'?([^'!]+)'?!\\$?([A-Z]+)\\$?(\\d+)\")\n",
        "ARRAY_REF_RE = re.compile(r\"=TRANSPOSE\\('?([^'!]+)'?!\\$?([A-Z]+)\\$?(\\d+):\\$?([A-Z]+)\\$?(\\d+)\\)\")\n",
        "ID_RE = re.compile(r\"Row(\\d+)([a-z]?)Cell(\\d+)([a-z]?)$\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "def normalize_sub_key(value):\n",
        "    if value is None or pd.isna(value):\n",
        "        return None\n",
        "    text = str(value).strip()\n",
        "    if text == \"\" or text.lower() == \"nan\":\n",
        "        return None\n",
        "    if text.endswith(\".0\"):\n",
        "        text = text[:-2]\n",
        "    return text.replace(\",\", \".\")\n",
        "\n",
        "\n",
        "def suffix_rank(value):\n",
        "    if not value:\n",
        "        return 0\n",
        "    return ord(value.lower()) - ord(\"a\") + 1\n",
        "\n",
        "\n",
        "def parse_id_sort_key(id_value):\n",
        "    if not isinstance(id_value, str):\n",
        "        id_value = str(id_value)\n",
        "    match = ID_RE.search(id_value)\n",
        "    if not match:\n",
        "        return (10**9, 0, 10**9, 0, id_value)\n",
        "    row_num = int(match.group(1))\n",
        "    row_suffix = suffix_rank(match.group(2))\n",
        "    cell_num = int(match.group(3))\n",
        "    cell_suffix = suffix_rank(match.group(4))\n",
        "    return (row_num, row_suffix, cell_num, cell_suffix, id_value)\n",
        "\n",
        "\n",
        "def sanitize_filename(text):\n",
        "    normalized = unicodedata.normalize(\"NFKD\", str(text)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    cleaned = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", normalized).strip(\"_\")\n",
        "    return cleaned or \"institution\"\n",
        "\n",
        "\n",
        "def expand_range(start_col, start_row, end_col, end_row):\n",
        "    c1 = column_index_from_string(start_col)\n",
        "    c2 = column_index_from_string(end_col)\n",
        "    r1 = int(start_row)\n",
        "    r2 = int(end_row)\n",
        "    if c2 < c1:\n",
        "        c1, c2 = c2, c1\n",
        "    if r2 < r1:\n",
        "        r1, r2 = r2, r1\n",
        "    cells = []\n",
        "    for row in range(r1, r2 + 1):\n",
        "        for col in range(c1, c2 + 1):\n",
        "            cells.append(f\"{get_column_letter(col)}{row}\")\n",
        "    return cells\n",
        "\n",
        "\n",
        "def parse_direct_ref(formula):\n",
        "    match = DIRECT_REF_RE.match(str(formula).strip())\n",
        "    if not match:\n",
        "        return None, None\n",
        "    sheet = match.group(1).strip(\"'\")\n",
        "    cell = f\"{match.group(2)}{match.group(3)}\"\n",
        "    return sheet, cell\n",
        "\n",
        "\n",
        "def parse_array_formula(text):\n",
        "    match = ARRAY_REF_RE.match(str(text).strip())\n",
        "    if not match:\n",
        "        return None, []\n",
        "    sheet = match.group(1).strip(\"'\")\n",
        "    cells = expand_range(match.group(2), match.group(3), match.group(4), match.group(5))\n",
        "    return sheet, cells\n",
        "\n",
        "\n",
        "def build_mapping_from_sheet(ws):\n",
        "    row_to_target = {}\n",
        "    seen_refs = set()\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, ArrayFormula):\n",
        "            if value.ref in seen_refs:\n",
        "                continue\n",
        "            seen_refs.add(value.ref)\n",
        "            ref_match = re.match(r\"B(\\d+):B(\\d+)\", value.ref)\n",
        "            if not ref_match:\n",
        "                continue\n",
        "            start_row = int(ref_match.group(1))\n",
        "            end_row = int(ref_match.group(2))\n",
        "            sheet, cells = parse_array_formula(value.text)\n",
        "            if not sheet or not cells:\n",
        "                continue\n",
        "            for idx, target_row in enumerate(range(start_row, end_row + 1)):\n",
        "                if idx < len(cells):\n",
        "                    row_to_target[target_row] = (sheet, cells[idx])\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        if row in row_to_target:\n",
        "            continue\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, str) and value.startswith(\"=\"):\n",
        "            sheet, cell = parse_direct_ref(value)\n",
        "            if sheet and cell:\n",
        "                row_to_target[row] = (sheet, cell)\n",
        "\n",
        "    inst_target = None\n",
        "    sub_map = {}\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        key = ws.cell(row=row, column=1).value\n",
        "        sub = ws.cell(row=row, column=3).value\n",
        "        if key == \"Inst\":\n",
        "            inst_target = row_to_target.get(row)\n",
        "            continue\n",
        "        if key and sub is not None:\n",
        "            sub_key = normalize_sub_key(sub)\n",
        "            if not sub_key:\n",
        "                continue\n",
        "            target = row_to_target.get(row)\n",
        "            if not target:\n",
        "                continue\n",
        "            sub_map.setdefault(sub_key, []).append(target)\n",
        "\n",
        "    return inst_target, sub_map\n",
        "\n",
        "\n",
        "def build_mapping_rows(ws):\n",
        "    row_to_target = {}\n",
        "    seen_refs = set()\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, ArrayFormula):\n",
        "            if value.ref in seen_refs:\n",
        "                continue\n",
        "            seen_refs.add(value.ref)\n",
        "            ref_match = re.match(r\"B(\\d+):B(\\d+)\", value.ref)\n",
        "            if not ref_match:\n",
        "                continue\n",
        "            start_row = int(ref_match.group(1))\n",
        "            end_row = int(ref_match.group(2))\n",
        "            sheet, cells = parse_array_formula(value.text)\n",
        "            if not sheet or not cells:\n",
        "                continue\n",
        "            for idx, target_row in enumerate(range(start_row, end_row + 1)):\n",
        "                if idx < len(cells):\n",
        "                    row_to_target[target_row] = (sheet, cells[idx])\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        if row in row_to_target:\n",
        "            continue\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, str) and value.startswith(\"=\"):\n",
        "            sheet, cell = parse_direct_ref(value)\n",
        "            if sheet and cell:\n",
        "                row_to_target[row] = (sheet, cell)\n",
        "\n",
        "    rows = []\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        key = ws.cell(row=row, column=1).value\n",
        "        sub = ws.cell(row=row, column=3).value\n",
        "        if not key or key in (\"Inst\", \"Report\") or sub is None:\n",
        "            continue\n",
        "        sub_key = normalize_sub_key(sub)\n",
        "        target = row_to_target.get(row)\n",
        "        if not sub_key or not target:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"sub_key\": sub_key,\n",
        "            \"mapping_key\": key,\n",
        "            \"target_sheet\": target[0],\n",
        "            \"target_cell\": target[1],\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def is_formula_cell(ws, cell):\n",
        "    target = ws[cell]\n",
        "    if target.data_type == \"f\":\n",
        "        return True\n",
        "    value = target.value\n",
        "    return isinstance(value, str) and value.startswith(\"=\")\n",
        "\n",
        "\n",
        "def build_consolidated_csv(files=None, output=CSV_FALLBACK_PATH):\n",
        "    if files is None:\n",
        "        files = sorted(Path('.').glob('combined_supplemental_form_data_long_*.csv'))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(\"No combined_supplemental_form_data_long_*.csv files found.\")\n",
        "    frames = [pd.read_csv(path) for path in files]\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "    df.to_csv(output, index=False)\n",
        "    print(f\"Wrote consolidated CSV to {output}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_supplemental_data(csv_url=None, fallback_path=CSV_FALLBACK_PATH):\n",
        "    df = None\n",
        "    if csv_url:\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                csv_url,\n",
        "                dtype={\"section_number\": \"string\", \"subsection_number\": \"string\"},\n",
        "                low_memory=False,\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            print(f\"Failed to read CSV from URL ({exc}); falling back to {fallback_path}.\")\n",
        "\n",
        "    if df is None:\n",
        "        if not Path(fallback_path).exists():\n",
        "            df = build_consolidated_csv(output=fallback_path)\n",
        "        else:\n",
        "            df = pd.read_csv(\n",
        "                fallback_path,\n",
        "                dtype={\"section_number\": \"string\", \"subsection_number\": \"string\"},\n",
        "                low_memory=False,\n",
        "            )\n",
        "\n",
        "    df[\"ReportingPeriodStart\"] = pd.to_datetime(df[\"ReportingPeriodStart\"], errors=\"coerce\")\n",
        "    df[\"ReportingPeriodEnd\"] = pd.to_datetime(df[\"ReportingPeriodEnd\"], errors=\"coerce\")\n",
        "    df[\"report_start\"] = df[\"ReportingPeriodStart\"].dt.date\n",
        "    df[\"report_end\"] = df[\"ReportingPeriodEnd\"].dt.date\n",
        "\n",
        "    df[\"gc_orgID\"] = pd.to_numeric(df[\"gc_orgID\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    sub = df[\"subsection_number\"].fillna(\"\").astype(\"string\")\n",
        "    sub = sub.where(~sub.str.lower().eq(\"nan\"), \"\")\n",
        "    sec = df[\"section_number\"].fillna(\"\").astype(\"string\")\n",
        "    sec = sec.where(~sec.str.lower().eq(\"nan\"), \"\")\n",
        "\n",
        "    sub_key = sub.mask(sub == \"\", sec)\n",
        "    df[\"sub_key\"] = sub_key.map(normalize_sub_key)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_output_filename(inst_name, start_date, lang):\n",
        "    year = pd.to_datetime(start_date, errors=\"coerce\").year\n",
        "    year_label = f\"{year}\" if pd.notna(year) else \"unknown\"\n",
        "    inst_slug = sanitize_filename(inst_name)[:60]\n",
        "    lang_label = \"EN\" if str(lang).lower().startswith(\"en\") else \"FR\"\n",
        "    return f\"Supplemental_Report_{year_label}_{lang_label}_{inst_slug}.xlsx\"\n",
        "\n",
        "\n",
        "def populate_workbook(wb, subset, inst_name, start_date, end_date, lang):\n",
        "    mapping_ws = wb[MAPPING_SHEET]\n",
        "    inst_target, sub_map = build_mapping_from_sheet(mapping_ws)\n",
        "\n",
        "    if inst_target:\n",
        "        inst_sheet, inst_cell = inst_target\n",
        "    else:\n",
        "        inst_sheet = SUPP_SHEET_EN if str(lang).lower().startswith(\"en\") else SUPP_SHEET_FR\n",
        "        inst_cell = \"D7\"\n",
        "\n",
        "    wb[inst_sheet][inst_cell] = inst_name\n",
        "\n",
        "    if pd.notna(start_date):\n",
        "        wb[inst_sheet][SUPP_DATE_START_CELL] = pd.to_datetime(start_date).date()\n",
        "    if pd.notna(end_date):\n",
        "        wb[inst_sheet][SUPP_DATE_END_CELL] = pd.to_datetime(end_date).date()\n",
        "\n",
        "    warnings = []\n",
        "    skipped_formula = 0\n",
        "    skipped_examples = []\n",
        "\n",
        "    for sub_key, targets in sub_map.items():\n",
        "        sub_rows = subset[subset[\"sub_key\"] == sub_key]\n",
        "        if sub_rows.empty:\n",
        "            warnings.append(f\"Subsection {sub_key}: no matching rows in data.\")\n",
        "            continue\n",
        "        sub_rows = sub_rows.copy()\n",
        "        sub_rows[\"sort_key\"] = sub_rows[\"id\"].map(parse_id_sort_key)\n",
        "        sub_rows = sub_rows.sort_values(\"sort_key\")\n",
        "        values = sub_rows[\"value\"].tolist()\n",
        "\n",
        "        if len(values) != len(targets):\n",
        "            warnings.append(\n",
        "                f\"Subsection {sub_key}: template has {len(targets)} cells, data has {len(values)}; filled {min(len(values), len(targets))}.\"\n",
        "            )\n",
        "\n",
        "        for value, target in zip(values, targets):\n",
        "            if pd.isna(value):\n",
        "                continue\n",
        "            sheet_name, cell = target\n",
        "            if is_formula_cell(wb[sheet_name], cell):\n",
        "                skipped_formula += 1\n",
        "                if len(skipped_examples) < 10:\n",
        "                    skipped_examples.append(f\"{sheet_name}!{cell}\")\n",
        "                continue\n",
        "            wb[sheet_name][cell] = value\n",
        "\n",
        "    if skipped_formula:\n",
        "        sample = \", \".join(skipped_examples)\n",
        "        warnings.append(\n",
        "            f\"Skipped {skipped_formula} formula cells (auto-calculated), e.g. {sample}.\"\n",
        "        )\n",
        "\n",
        "    return warnings\n",
        "\n",
        "\n",
        "def generate_supplemental_report(df, gc_org_id, report_start, lang=\"En\", output_path=None):\n",
        "    report_start = pd.to_datetime(report_start, errors=\"coerce\").date()\n",
        "\n",
        "    template_path = TEMPLATE_EN if str(lang).lower().startswith(\"en\") else TEMPLATE_FR\n",
        "    if not Path(template_path).exists():\n",
        "        raise FileNotFoundError(f\"Template not found: {template_path}\")\n",
        "\n",
        "    subset = df[(df[\"gc_orgID\"] == gc_org_id) & (df[\"report_start\"] == report_start)]\n",
        "    if subset.empty:\n",
        "        raise ValueError(\"No rows found for the selected institution and period.\")\n",
        "\n",
        "    first_row = subset.iloc[0]\n",
        "    inst_name = first_row[\"institution_en\"] if str(lang).lower().startswith(\"en\") else first_row[\"institution_fr\"]\n",
        "    start_date = first_row[\"ReportingPeriodStart\"]\n",
        "    end_date = first_row[\"ReportingPeriodEnd\"]\n",
        "\n",
        "    wb = load_workbook(template_path, data_only=False)\n",
        "    warnings = populate_workbook(wb, subset, inst_name, start_date, end_date, lang)\n",
        "\n",
        "    if output_path is None:\n",
        "        output_path = build_output_filename(inst_name, start_date, lang)\n",
        "\n",
        "    wb.save(output_path)\n",
        "    return output_path, warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_supplemental_data(CSV_URL, CSV_FALLBACK_PATH)\n",
        "print(f\"Loaded {len(df):,} rows from supplemental CSV\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example (manual usage without widgets):\n",
        "# output_path, warnings = generate_supplemental_report(\n",
        "#     df,\n",
        "#     gc_org_id=1234,\n",
        "#     report_start=\"2024-04-01\",\n",
        "#     lang=\"En\",\n",
        "# )\n",
        "# print(\"Saved:\", output_path)\n",
        "# if warnings:\n",
        "#     print(\"Warnings:\")\n",
        "#     for item in warnings[:20]:\n",
        "#         print(\"-\", item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import ipywidgets as widgets\n",
        "\n",
        "    inst_df = (\n",
        "        df[[\"gc_orgID\", \"institution_en\", \"institution_fr\"]]\n",
        "        .dropna(subset=[\"gc_orgID\"])\n",
        "        .drop_duplicates()\n",
        "        .sort_values(\"gc_orgID\")\n",
        "    )\n",
        "    inst_df[\"gc_orgID\"] = inst_df[\"gc_orgID\"].astype(int)\n",
        "\n",
        "    inst_options = [\n",
        "        (f\"{row.gc_orgID} | {row.institution_en}\", row.gc_orgID)\n",
        "        for row in inst_df.itertuples(index=False)\n",
        "    ]\n",
        "\n",
        "    period_dates = sorted(df[\"report_start\"].dropna().unique())\n",
        "    period_options = [(f\"{d.year}-{d.year + 1}\", d) for d in period_dates]\n",
        "\n",
        "    inst_dropdown = widgets.Dropdown(options=inst_options, description=\"Institution\", layout=widgets.Layout(width=\"80%\"))\n",
        "    period_dropdown = widgets.Dropdown(options=period_options, description=\"Period\")\n",
        "    lang_dropdown = widgets.Dropdown(options=[\"En\", \"Fr\"], description=\"Language\")\n",
        "    generate_btn = widgets.Button(description=\"Generate XLSX\", button_style=\"primary\")\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_click(_):\n",
        "        output_area.clear_output()\n",
        "        with output_area:\n",
        "            try:\n",
        "                output_path, warnings = generate_supplemental_report(\n",
        "                    df,\n",
        "                    gc_org_id=inst_dropdown.value,\n",
        "                    report_start=period_dropdown.value,\n",
        "                    lang=lang_dropdown.value,\n",
        "                )\n",
        "                print(f\"Saved: {output_path}\")\n",
        "                if warnings:\n",
        "                    print(\"Warnings:\")\n",
        "                    for item in warnings[:20]:\n",
        "                        print(\"-\", item)\n",
        "            except Exception as exc:\n",
        "                print(\"Error:\", exc)\n",
        "\n",
        "    generate_btn.on_click(on_click)\n",
        "    display(widgets.VBox([inst_dropdown, period_dropdown, lang_dropdown, generate_btn, output_area]))\n",
        "except ImportError:\n",
        "    print(\"ipywidgets not installed. Run %pip install ipywidgets, or call generate_supplemental_report(...) manually.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mapping accuracy (dataset headers vs XLSX labels)\n",
        "\n",
        "This cell builds a field-level accuracy estimate based on dataset header similarity to spreadsheet labels derived from the template.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ACCURACY_LANG = \"En\"  # Set to \"Fr\" to evaluate against the French template\n",
        "\n",
        "\n",
        "def build_dataset_headers(url):\n",
        "    raw = pd.read_excel(url, header=None)\n",
        "    header_rows = raw.iloc[:3].copy()\n",
        "    header_rows = header_rows.ffill(axis=1)\n",
        "    headers = []\n",
        "    for col in header_rows.columns:\n",
        "        parts = []\n",
        "        for row in range(3):\n",
        "            val = header_rows.iat[row, col]\n",
        "            if pd.isna(val):\n",
        "                continue\n",
        "            text = str(val).strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            if not parts or parts[-1] != text:\n",
        "                parts.append(text)\n",
        "        headers.append(' - '.join(parts) if parts else None)\n",
        "    if headers and headers[0] is None:\n",
        "        headers[0] = 'Institution'\n",
        "    return headers\n",
        "\n",
        "\n",
        "def similarity(a, b):\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "    return SequenceMatcher(None, str(a).casefold(), str(b).casefold()).ratio()\n",
        "\n",
        "\n",
        "def find_row_label(ws, cell):\n",
        "    target = ws[cell]\n",
        "    for col in range(target.column - 1, 0, -1):\n",
        "        value = ws.cell(row=target.row, column=col).value\n",
        "        if isinstance(value, str) and value.strip():\n",
        "            return value.strip()\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_col_label(ws, cell):\n",
        "    target = ws[cell]\n",
        "    for row in range(target.row - 1, 0, -1):\n",
        "        value = ws.cell(row=row, column=target.column).value\n",
        "        if isinstance(value, str) and value.strip():\n",
        "            return value.strip()\n",
        "    return None\n",
        "\n",
        "\n",
        "schema = pd.read_excel(SCHEMA_PATH)\n",
        "value_rows = schema[~schema['id'].isin(['NameOfInstitution', 'ReportingPeriodStart', 'ReportingPeriodEnd'])].copy()\n",
        "value_rows = value_rows.reset_index(drop=True)\n",
        "value_rows['schema_index'] = value_rows.index\n",
        "value_rows['sub_key'] = value_rows['subsection_number'].apply(normalize_sub_key)\n",
        "value_rows['sub_key'] = value_rows['sub_key'].where(value_rows['sub_key'].notna(), value_rows['section_number'].apply(normalize_sub_key))\n",
        "value_rows['id_sort'] = value_rows['id'].map(parse_id_sort_key)\n",
        "\n",
        "value_rows_sorted = value_rows.sort_values(['sub_key', 'id_sort', 'schema_index']).copy()\n",
        "value_rows_sorted['sub_index'] = value_rows_sorted.groupby('sub_key').cumcount()\n",
        "\n",
        "accuracy_template = TEMPLATE_EN if str(ACCURACY_LANG).lower().startswith(\"en\") else TEMPLATE_FR\n",
        "wb = load_workbook(accuracy_template, data_only=False)\n",
        "map_ws = wb[MAPPING_SHEET]\n",
        "\n",
        "mapping_rows = build_mapping_rows(map_ws)\n",
        "mapping_lookup = {}\n",
        "for sub_key in sorted({row['sub_key'] for row in mapping_rows}):\n",
        "    rows = [r for r in mapping_rows if r['sub_key'] == sub_key]\n",
        "    for idx, row in enumerate(rows):\n",
        "        mapping_lookup[(sub_key, idx)] = row\n",
        "\n",
        "headers = build_dataset_headers(DATASET_HEADER_URL)\n",
        "\n",
        "rows = []\n",
        "for row in value_rows_sorted.itertuples(index=False):\n",
        "    sub_key = row.sub_key\n",
        "    sub_index = row.sub_index\n",
        "    mapping = mapping_lookup.get((sub_key, sub_index))\n",
        "\n",
        "    dataset_col_index = row.schema_index + 1\n",
        "    dataset_col_name = headers[dataset_col_index] if dataset_col_index < len(headers) else None\n",
        "\n",
        "    if mapping:\n",
        "        target_sheet = mapping['target_sheet']\n",
        "        target_cell = mapping['target_cell']\n",
        "        target_is_formula = is_formula_cell(wb[target_sheet], target_cell)\n",
        "        row_label = find_row_label(wb[target_sheet], target_cell)\n",
        "        col_label = find_col_label(wb[target_sheet], target_cell)\n",
        "        combined_label = \" - \".join([t for t in [row_label, col_label] if t])\n",
        "        mapping_key = mapping['mapping_key']\n",
        "    else:\n",
        "        target_sheet = None\n",
        "        target_cell = None\n",
        "        target_is_formula = None\n",
        "        row_label = None\n",
        "        col_label = None\n",
        "        combined_label = None\n",
        "        mapping_key = None\n",
        "\n",
        "    sim_combined = similarity(dataset_col_name, combined_label)\n",
        "    sim_row = similarity(dataset_col_name, row_label)\n",
        "    sim_col = similarity(dataset_col_name, col_label)\n",
        "    estimated = max(sim_combined, sim_row, sim_col)\n",
        "\n",
        "    rows.append({\n",
        "        'dataset_column_index': dataset_col_index,\n",
        "        'dataset_column_name': dataset_col_name,\n",
        "        'html_form_id': row.id,\n",
        "        'section_number': row.section_number,\n",
        "        'subsection_number': row.subsection_number,\n",
        "        'sub_key': sub_key,\n",
        "        'mapping_code_for_consumption': mapping_key,\n",
        "        'xlsx_sheet': target_sheet,\n",
        "        'xlsx_cell': target_cell,\n",
        "        'xlsx_ref': f\"{target_sheet}!{target_cell}\" if target_sheet and target_cell else None,\n",
        "        'xlsx_cell_is_formula': target_is_formula,\n",
        "        'xlsx_row_label': row_label,\n",
        "        'xlsx_col_label': col_label,\n",
        "        'xlsx_label_combined': combined_label,\n",
        "        'mapping_status': 'ok' if mapping else 'missing_xlsx_mapping',\n",
        "        'header_similarity': sim_combined,\n",
        "        'row_label_similarity': sim_row,\n",
        "        'col_label_similarity': sim_col,\n",
        "        'estimated_accuracy': estimated,\n",
        "    })\n",
        "\n",
        "accuracy_df = pd.DataFrame(rows)\n",
        "accuracy_df['accuracy_band'] = pd.cut(\n",
        "    accuracy_df['estimated_accuracy'],\n",
        "    bins=[-0.01, 0.35, 0.6, 1.01],\n",
        "    labels=['low', 'medium', 'high'],\n",
        ")\n",
        "\n",
        "accuracy_path = Path('supplemental_rosetta_mapping_accuracy.csv')\n",
        "accuracy_df.to_csv(accuracy_path, index=False)\n",
        "\n",
        "mismatch_df = accuracy_df[(accuracy_df['mapping_status'] != 'ok') | (accuracy_df['estimated_accuracy'] < 0.35)].copy()\n",
        "mismatch_path = Path('supplemental_rosetta_mismatches.csv')\n",
        "mismatch_df.to_csv(mismatch_path, index=False)\n",
        "\n",
        "print(f\"Wrote {accuracy_path}\")\n",
        "print(f\"Wrote {mismatch_path}\")\n",
        "print(\"Summary:\")\n",
        "print(accuracy_df['accuracy_band'].value_counts().to_string())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}