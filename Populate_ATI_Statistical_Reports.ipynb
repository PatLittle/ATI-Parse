{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Populate ATI Tab from ATI-AI_refactored.csv\n",
        "\n",
        "This notebook fills the ATI/LAI sheet in the 2024-2025 Statistical Reports template from the consolidated ATI dataset.\n",
        "\n",
        "- Select institution, reporting period, and language.\n",
        "- Generates a new .xlsx file with the ATI/LAI tab populated.\n",
        "- Set `CSV_URL` when the consolidated CSV is hosted online.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install ipywidgets\n",
        "import re\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.utils import column_index_from_string, get_column_letter\n",
        "from openpyxl.worksheet.formula import ArrayFormula\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "CSV_URL = None  # TODO: set to hosted URL (e.g., https://.../ATI-AI_refactored.csv)\n",
        "CSV_FALLBACK_PATH = \"ATI-AI_refactored.csv\"\n",
        "\n",
        "TEMPLATE_EN = \"2024-2025 Statistical Reports.xlsx\"\n",
        "TEMPLATE_FR = \"2024-2025 Rapports statistiques.xlsx\"\n",
        "MAPPING_SHEET = \"ATI_ForConsumption\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "DIRECT_REF_RE = re.compile(r\"=\\s*'?([^'!]+)'?!\\$?([A-Z]+)\\$?(\\d+)\")\n",
        "ARRAY_REF_RE = re.compile(r\"=TRANSPOSE\\('?([^'!]+)'?!\\$?([A-Z]+)\\$?(\\d+):\\$?([A-Z]+)\\$?(\\d+)\\)\")\n",
        "ID_RE = re.compile(r\"Row(\\d+)([a-z]?)Cell(\\d+)([a-z]?)$\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "def normalize_sub_key(value):\n",
        "    if value is None or pd.isna(value):\n",
        "        return None\n",
        "    text = str(value).strip()\n",
        "    if text == \"\" or text.lower() == \"nan\":\n",
        "        return None\n",
        "    if text.endswith(\".0\"):\n",
        "        text = text[:-2]\n",
        "    return text.replace(\",\", \".\")\n",
        "\n",
        "\n",
        "def suffix_rank(value):\n",
        "    if not value:\n",
        "        return 0\n",
        "    return ord(value.lower()) - ord(\"a\") + 1\n",
        "\n",
        "\n",
        "def parse_id_sort_key(id_value):\n",
        "    if not isinstance(id_value, str):\n",
        "        id_value = str(id_value)\n",
        "    match = ID_RE.search(id_value)\n",
        "    if not match:\n",
        "        return (10**9, 0, 10**9, 0, id_value)\n",
        "    row_num = int(match.group(1))\n",
        "    row_suffix = suffix_rank(match.group(2))\n",
        "    cell_num = int(match.group(3))\n",
        "    cell_suffix = suffix_rank(match.group(4))\n",
        "    return (row_num, row_suffix, cell_num, cell_suffix, id_value)\n",
        "\n",
        "\n",
        "def sanitize_filename(text):\n",
        "    normalized = unicodedata.normalize(\"NFKD\", str(text)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    cleaned = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", normalized).strip(\"_\")\n",
        "    return cleaned or \"institution\"\n",
        "\n",
        "\n",
        "def expand_range(start_col, start_row, end_col, end_row):\n",
        "    c1 = column_index_from_string(start_col)\n",
        "    c2 = column_index_from_string(end_col)\n",
        "    r1 = int(start_row)\n",
        "    r2 = int(end_row)\n",
        "    if c2 < c1:\n",
        "        c1, c2 = c2, c1\n",
        "    if r2 < r1:\n",
        "        r1, r2 = r2, r1\n",
        "    cells = []\n",
        "    for row in range(r1, r2 + 1):\n",
        "        for col in range(c1, c2 + 1):\n",
        "            cells.append(f\"{get_column_letter(col)}{row}\")\n",
        "    return cells\n",
        "\n",
        "\n",
        "def parse_direct_ref(formula):\n",
        "    match = DIRECT_REF_RE.match(str(formula).strip())\n",
        "    if not match:\n",
        "        return None, None\n",
        "    sheet = match.group(1).strip(\"'\")\n",
        "    cell = f\"{match.group(2)}{match.group(3)}\"\n",
        "    return sheet, cell\n",
        "\n",
        "\n",
        "def parse_array_formula(text):\n",
        "    match = ARRAY_REF_RE.match(str(text).strip())\n",
        "    if not match:\n",
        "        return None, []\n",
        "    sheet = match.group(1).strip(\"'\")\n",
        "    cells = expand_range(match.group(2), match.group(3), match.group(4), match.group(5))\n",
        "    return sheet, cells\n",
        "\n",
        "\n",
        "def build_mapping_from_sheet(ws):\n",
        "    row_to_target = {}\n",
        "    seen_refs = set()\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, ArrayFormula):\n",
        "            if value.ref in seen_refs:\n",
        "                continue\n",
        "            seen_refs.add(value.ref)\n",
        "            ref_match = re.match(r\"B(\\d+):B(\\d+)\", value.ref)\n",
        "            if not ref_match:\n",
        "                continue\n",
        "            start_row = int(ref_match.group(1))\n",
        "            end_row = int(ref_match.group(2))\n",
        "            sheet, cells = parse_array_formula(value.text)\n",
        "            if not sheet or not cells:\n",
        "                continue\n",
        "            for idx, target_row in enumerate(range(start_row, end_row + 1)):\n",
        "                if idx < len(cells):\n",
        "                    row_to_target[target_row] = (sheet, cells[idx])\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        if row in row_to_target:\n",
        "            continue\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, str) and value.startswith(\"=\"):\n",
        "            sheet, cell = parse_direct_ref(value)\n",
        "            if sheet and cell:\n",
        "                row_to_target[row] = (sheet, cell)\n",
        "\n",
        "    inst_target = None\n",
        "    sub_map = {}\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        key = ws.cell(row=row, column=1).value\n",
        "        sub = ws.cell(row=row, column=3).value\n",
        "        if key == \"Inst\":\n",
        "            inst_target = row_to_target.get(row)\n",
        "            continue\n",
        "        if key and sub is not None:\n",
        "            sub_key = normalize_sub_key(sub)\n",
        "            if not sub_key:\n",
        "                continue\n",
        "            target = row_to_target.get(row)\n",
        "            if not target:\n",
        "                continue\n",
        "            sub_map.setdefault(sub_key, []).append(target)\n",
        "\n",
        "    return inst_target, sub_map\n",
        "\n",
        "\n",
        "def load_ati_data(csv_url=None, fallback_path=CSV_FALLBACK_PATH):\n",
        "    df = None\n",
        "    if csv_url:\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                csv_url,\n",
        "                dtype={\"section_number\": \"string\", \"subsection_number\": \"string\"},\n",
        "                low_memory=False,\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            print(f\"Failed to read CSV from URL ({exc}); falling back to {fallback_path}.\")\n",
        "\n",
        "    if df is None:\n",
        "        df = pd.read_csv(\n",
        "            fallback_path,\n",
        "            dtype={\"section_number\": \"string\", \"subsection_number\": \"string\"},\n",
        "            low_memory=False,\n",
        "        )\n",
        "\n",
        "    df[\"ReportingPeriodStart\"] = pd.to_datetime(df[\"ReportingPeriodStart\"], errors=\"coerce\")\n",
        "    df[\"ReportingPeriodEnd\"] = pd.to_datetime(df[\"ReportingPeriodEnd\"], errors=\"coerce\")\n",
        "    df[\"report_start\"] = df[\"ReportingPeriodStart\"].dt.date\n",
        "    df[\"report_end\"] = df[\"ReportingPeriodEnd\"].dt.date\n",
        "\n",
        "    df[\"gc_orgID\"] = pd.to_numeric(df[\"gc_orgID\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    sub = df[\"subsection_number\"].fillna(\"\").astype(\"string\")\n",
        "    sub = sub.where(~sub.str.lower().eq(\"nan\"), \"\")\n",
        "    sec = df[\"section_number\"].fillna(\"\").astype(\"string\")\n",
        "    sec = sec.where(~sec.str.lower().eq(\"nan\"), \"\")\n",
        "\n",
        "    sub_key = sub.mask(sub == \"\", sec)\n",
        "    df[\"sub_key\"] = sub_key.map(normalize_sub_key)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_output_filename(inst_name, start_date, lang):\n",
        "    year = pd.to_datetime(start_date, errors=\"coerce\").year\n",
        "    year_label = f\"{year}\" if pd.notna(year) else \"unknown\"\n",
        "    inst_slug = sanitize_filename(inst_name)[:60]\n",
        "    lang_label = \"EN\" if str(lang).lower().startswith(\"en\") else \"FR\"\n",
        "    return f\"ATI_Report_{year_label}_{lang_label}_{inst_slug}.xlsx\"\n",
        "\n",
        "\n",
        "\n",
        "def is_formula_cell(ws, cell):\n",
        "    target = ws[cell]\n",
        "    if target.data_type == \"f\":\n",
        "        return True\n",
        "    value = target.value\n",
        "    return isinstance(value, str) and value.startswith(\"=\")\n",
        "\n",
        "def populate_workbook(wb, subset, inst_name, start_date, end_date, lang):\n",
        "    mapping_ws = wb[MAPPING_SHEET]\n",
        "    inst_target, sub_map = build_mapping_from_sheet(mapping_ws)\n",
        "\n",
        "    if inst_target:\n",
        "        inst_sheet, inst_cell = inst_target\n",
        "    else:\n",
        "        inst_sheet = \"ATI\" if str(lang).lower().startswith(\"en\") else \"LAI\"\n",
        "        inst_cell = \"D6\"\n",
        "\n",
        "    wb[inst_sheet][inst_cell] = inst_name\n",
        "\n",
        "    if pd.notna(start_date):\n",
        "        wb[inst_sheet][\"D8\"] = pd.to_datetime(start_date).date()\n",
        "    if pd.notna(end_date):\n",
        "        wb[inst_sheet][\"H8\"] = pd.to_datetime(end_date).date()\n",
        "\n",
        "    warnings = []\n",
        "\n",
        "    skipped_formula = 0\n",
        "    skipped_examples = []\n",
        "\n",
        "    for sub_key, targets in sub_map.items():\n",
        "        sub_rows = subset[subset[\"sub_key\"] == sub_key]\n",
        "        if sub_rows.empty:\n",
        "            warnings.append(f\"Subsection {sub_key}: no matching rows in data.\")\n",
        "            continue\n",
        "        sub_rows = sub_rows.copy()\n",
        "        sub_rows[\"sort_key\"] = sub_rows[\"id\"].map(parse_id_sort_key)\n",
        "        sub_rows = sub_rows.sort_values(\"sort_key\")\n",
        "        values = sub_rows[\"value\"].tolist()\n",
        "\n",
        "        if len(values) != len(targets):\n",
        "            warnings.append(\n",
        "                f\"Subsection {sub_key}: template has {len(targets)} cells, data has {len(values)}; filled {min(len(values), len(targets))}.\"\n",
        "            )\n",
        "\n",
        "        for value, target in zip(values, targets):\n",
        "            if pd.isna(value):\n",
        "                continue\n",
        "            sheet_name, cell = target\n",
        "            if is_formula_cell(wb[sheet_name], cell):\n",
        "                skipped_formula += 1\n",
        "                if len(skipped_examples) < 10:\n",
        "                    skipped_examples.append(f\"{sheet_name}!{cell}\")\n",
        "                continue\n",
        "            wb[sheet_name][cell] = value\n",
        "\n",
        "    if skipped_formula:\n",
        "        sample = \", \".join(skipped_examples)\n",
        "        warnings.append(\n",
        "            f\"Skipped {skipped_formula} formula cells (auto-calculated), e.g. {sample}.\"\n",
        "        )\n",
        "\n",
        "    return warnings\n",
        "\n",
        "\n",
        "def generate_ati_report(df, gc_org_id, report_start, lang=\"En\", output_path=None):\n",
        "    report_start = pd.to_datetime(report_start, errors=\"coerce\").date()\n",
        "\n",
        "    template_path = TEMPLATE_EN if str(lang).lower().startswith(\"en\") else TEMPLATE_FR\n",
        "    if not Path(template_path).exists():\n",
        "        raise FileNotFoundError(f\"Template not found: {template_path}\")\n",
        "\n",
        "    subset = df[(df[\"gc_orgID\"] == gc_org_id) & (df[\"report_start\"] == report_start)]\n",
        "    if subset.empty:\n",
        "        raise ValueError(\"No rows found for the selected institution and period.\")\n",
        "\n",
        "    first_row = subset.iloc[0]\n",
        "    inst_name = first_row[\"institution_fr\"] if str(lang).lower().startswith(\"fr\") else first_row[\"institution_en\"]\n",
        "    start_date = first_row[\"ReportingPeriodStart\"]\n",
        "    end_date = first_row[\"ReportingPeriodEnd\"]\n",
        "\n",
        "    wb = load_workbook(template_path, data_only=False)\n",
        "    warnings = populate_workbook(wb, subset, inst_name, start_date, end_date, lang)\n",
        "\n",
        "    if output_path is None:\n",
        "        output_path = build_output_filename(inst_name, start_date, lang)\n",
        "\n",
        "    wb.save(output_path)\n",
        "    return output_path, warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 697,866 rows from ATI-AI_refactored.csv\n"
          ]
        }
      ],
      "source": [
        "df = load_ati_data(CSV_URL, CSV_FALLBACK_PATH)\n",
        "print(f\"Loaded {len(df):,} rows from ATI-AI_refactored.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example (manual usage without widgets):\n",
        "# output_path, warnings = generate_ati_report(\n",
        "#     df,\n",
        "#     gc_org_id=2297,\n",
        "#     report_start=\"2024-04-01\",\n",
        "#     lang=\"En\",\n",
        "# )\n",
        "# print(\"Saved:\", output_path)\n",
        "# if warnings:\n",
        "#     print(\"Warnings:\")\n",
        "#     for item in warnings[:20]:\n",
        "#         print(\"-\", item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "617399a138e64be7ae5313dd4998b154",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Dropdown(description='Institution', layout=Layout(width='80%'), options=(('2222 | Agriculture a\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    import ipywidgets as widgets\n",
        "\n",
        "    inst_df = (\n",
        "        df[[\"gc_orgID\", \"institution_en\", \"institution_fr\"]]\n",
        "        .dropna(subset=[\"gc_orgID\"])\n",
        "        .drop_duplicates()\n",
        "        .sort_values(\"gc_orgID\")\n",
        "    )\n",
        "    inst_df[\"gc_orgID\"] = inst_df[\"gc_orgID\"].astype(int)\n",
        "\n",
        "    inst_options = [\n",
        "        (f\"{row.gc_orgID} | {row.institution_en}\", row.gc_orgID)\n",
        "        for row in inst_df.itertuples(index=False)\n",
        "    ]\n",
        "\n",
        "    period_dates = sorted(df[\"report_start\"].dropna().unique())\n",
        "    period_options = [(f\"{d.year}-{d.year + 1}\", d) for d in period_dates]\n",
        "\n",
        "    inst_dropdown = widgets.Dropdown(options=inst_options, description=\"Institution\", layout=widgets.Layout(width=\"80%\"))\n",
        "    period_dropdown = widgets.Dropdown(options=period_options, description=\"Period\")\n",
        "    lang_dropdown = widgets.Dropdown(options=[\"En\", \"Fr\"], description=\"Language\")\n",
        "    generate_btn = widgets.Button(description=\"Generate XLSX\", button_style=\"primary\")\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_click(_):\n",
        "        output_area.clear_output()\n",
        "        with output_area:\n",
        "            try:\n",
        "                output_path, warnings = generate_ati_report(\n",
        "                    df,\n",
        "                    gc_org_id=inst_dropdown.value,\n",
        "                    report_start=period_dropdown.value,\n",
        "                    lang=lang_dropdown.value,\n",
        "                )\n",
        "                print(f\"Saved: {output_path}\")\n",
        "                if warnings:\n",
        "                    print(\"Warnings:\")\n",
        "                    for item in warnings[:20]:\n",
        "                        print(\"-\", item)\n",
        "            except Exception as exc:\n",
        "                print(\"Error:\", exc)\n",
        "\n",
        "    generate_btn.on_click(on_click)\n",
        "    display(widgets.VBox([inst_dropdown, period_dropdown, lang_dropdown, generate_btn, output_area]))\n",
        "except ImportError:\n",
        "    print(\"ipywidgets not installed. Run %pip install ipywidgets, or call generate_ati_report(...) manually.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import re\n",
        "\n",
        "\n",
        "def read_docx_text(path):\n",
        "    with zipfile.ZipFile(path) as zf:\n",
        "        xml = zf.read(\"word/document.xml\").decode(\"utf-8\", errors=\"ignore\")\n",
        "    text = re.sub(r\"<w:tab[^/]*/>\", \"\t\", xml)\n",
        "    text = re.sub(r\"</w:p>\", \"\n",
        "\", text)\n",
        "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
        "    text = text.replace(\"\r",
        "\", \"\")\n",
        "    lines = [line.strip() for line in text.split(\"\n",
        "\")]\n",
        "    return [line for line in lines if line]\n",
        "\n",
        "\n",
        "validation_text = read_docx_text(\"2024-2025 Validation Checklist.docx\")\n",
        "guide_text = read_docx_text(\"2024-25 Guide ATIA Statistical Report.docx\")\n",
        "\n",
        "print(f\"Validation checklist lines: {len(validation_text)}\")\n",
        "print(f\"Guide lines: {len(guide_text)}\")\n",
        "print(\"Rules checked in audit:\")\n",
        "print(\"- Validation checklist: page/minute >0 requires request count >0 (sections 2.4, 2.5, 4.5.2, 4.5.4, 4.5.6, 8.1, 8.2).\")\n",
        "print(\"- Validation checklist: Section 5.1 dispositions must appear in Section 4.1.\")\n",
        "print(\"- Guide: Section 1.1 totals and carryover relationships.\")\n",
        "print(\"- Guide: Section 1.2 and 1.3 totals equal sum of rows 1-6.\")\n",
        "print(\"- Guide: numeric-only values.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Q_KEY_RE = re.compile(r\"^Q(\\d+)R(\\d+)C(\\d+)$\")\n",
        "\n",
        "\n",
        "def parse_id_components(id_value):\n",
        "    match = ID_RE.search(str(id_value))\n",
        "    if not match:\n",
        "        return None\n",
        "    row_num = int(match.group(1))\n",
        "    row_suffix = match.group(2) or \"\"\n",
        "    cell_num = int(match.group(3))\n",
        "    cell_suffix = match.group(4) or \"\"\n",
        "    return row_num, row_suffix.lower(), cell_num, cell_suffix.lower()\n",
        "\n",
        "\n",
        "def build_row_to_target(ws):\n",
        "    row_to_target = {}\n",
        "    seen_refs = set()\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, ArrayFormula):\n",
        "            if value.ref in seen_refs:\n",
        "                continue\n",
        "            seen_refs.add(value.ref)\n",
        "            ref_match = re.match(r\"B(\\d+):B(\\d+)\", value.ref)\n",
        "            if not ref_match:\n",
        "                continue\n",
        "            start_row = int(ref_match.group(1))\n",
        "            end_row = int(ref_match.group(2))\n",
        "            sheet, cells = parse_array_formula(value.text)\n",
        "            if not sheet or not cells:\n",
        "                continue\n",
        "            for idx, target_row in enumerate(range(start_row, end_row + 1)):\n",
        "                if idx < len(cells):\n",
        "                    row_to_target[target_row] = (sheet, cells[idx])\n",
        "\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        if row in row_to_target:\n",
        "            continue\n",
        "        value = ws.cell(row=row, column=2).value\n",
        "        if isinstance(value, str) and value.startswith(\"=\"):\n",
        "            sheet, cell = parse_direct_ref(value)\n",
        "            if sheet and cell:\n",
        "                row_to_target[row] = (sheet, cell)\n",
        "\n",
        "    return row_to_target\n",
        "\n",
        "\n",
        "def build_mapping_rows(ws):\n",
        "    row_to_target = build_row_to_target(ws)\n",
        "    rows = []\n",
        "    for row in range(1, ws.max_row + 1):\n",
        "        key = ws.cell(row=row, column=1).value\n",
        "        sub = ws.cell(row=row, column=3).value\n",
        "        if not key or key in (\"Inst\", \"Report\") or sub is None:\n",
        "            continue\n",
        "        sub_key = normalize_sub_key(sub)\n",
        "        target = row_to_target.get(row)\n",
        "        if not sub_key or not target:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"mapping_row\": row,\n",
        "            \"mapping_key\": key,\n",
        "            \"sub_key\": sub_key,\n",
        "            \"target_sheet\": target[0],\n",
        "            \"target_cell\": target[1],\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def build_data_rows(subset):\n",
        "    data_rows = {}\n",
        "    tmp = subset.copy()\n",
        "    tmp[\"id_sort\"] = tmp[\"id\"].map(parse_id_sort_key)\n",
        "    tmp = tmp.sort_values([\"sub_key\", \"id_sort\"])\n",
        "\n",
        "    for sub_key, group in tmp.groupby(\"sub_key\", sort=False):\n",
        "        rows = []\n",
        "        for row in group.itertuples(index=False):\n",
        "            comp = parse_id_components(row.id)\n",
        "            if not comp:\n",
        "                continue\n",
        "            row_num, row_suffix, cell_num, cell_suffix = comp\n",
        "            rows.append({\n",
        "                \"id\": row.id,\n",
        "                \"value\": row.value,\n",
        "                \"row_num\": row_num,\n",
        "                \"row_suffix\": row_suffix,\n",
        "                \"cell_num\": cell_num,\n",
        "                \"cell_suffix\": cell_suffix,\n",
        "            })\n",
        "        data_rows[sub_key] = rows\n",
        "    return data_rows\n",
        "\n",
        "\n",
        "def audit_mapping(df, gc_org_id, report_start, lang=\"En\", output_prefix=\"ati_audit\"):\n",
        "    report_start = pd.to_datetime(report_start, errors=\"coerce\").date()\n",
        "    subset = df[(df[\"gc_orgID\"] == gc_org_id) & (df[\"report_start\"] == report_start)]\n",
        "    if subset.empty:\n",
        "        raise ValueError(\"No rows found for the selected institution and period.\")\n",
        "\n",
        "    template_path = TEMPLATE_EN if str(lang).lower().startswith(\"en\") else TEMPLATE_FR\n",
        "    wb = load_workbook(template_path, data_only=False)\n",
        "    mapping_ws = wb[MAPPING_SHEET]\n",
        "\n",
        "    mapping_rows = build_mapping_rows(mapping_ws)\n",
        "    data_rows = build_data_rows(subset)\n",
        "\n",
        "    sub_index = {}\n",
        "    audit_rows = []\n",
        "\n",
        "    for mapping in mapping_rows:\n",
        "        sub_key = mapping[\"sub_key\"]\n",
        "        idx = sub_index.get(sub_key, 0)\n",
        "        sub_index[sub_key] = idx + 1\n",
        "\n",
        "        rows = data_rows.get(sub_key, [])\n",
        "        data_row = rows[idx] if idx < len(rows) else None\n",
        "\n",
        "        target_sheet = mapping[\"target_sheet\"]\n",
        "        target_cell = mapping[\"target_cell\"]\n",
        "        is_formula = is_formula_cell(wb[target_sheet], target_cell)\n",
        "\n",
        "        audit_rows.append({\n",
        "            \"mapping_row\": mapping[\"mapping_row\"],\n",
        "            \"mapping_key\": mapping[\"mapping_key\"],\n",
        "            \"sub_key\": sub_key,\n",
        "            \"target_sheet\": target_sheet,\n",
        "            \"target_cell\": target_cell,\n",
        "            \"target_is_formula\": is_formula,\n",
        "            \"csv_id\": data_row[\"id\"] if data_row else None,\n",
        "            \"csv_value\": data_row[\"value\"] if data_row else None,\n",
        "            \"csv_row_num\": data_row[\"row_num\"] if data_row else None,\n",
        "            \"csv_row_suffix\": data_row[\"row_suffix\"] if data_row else None,\n",
        "            \"csv_cell_num\": data_row[\"cell_num\"] if data_row else None,\n",
        "            \"csv_cell_suffix\": data_row[\"cell_suffix\"] if data_row else None,\n",
        "            \"mapping_status\": \"formula_cell\" if is_formula else (\"ok\" if data_row else \"missing_data\"),\n",
        "        })\n",
        "\n",
        "    audit_df = pd.DataFrame(audit_rows)\n",
        "\n",
        "    unmapped = []\n",
        "    for sub_key, rows in data_rows.items():\n",
        "        mapped_count = sum(1 for row in audit_rows if row[\"sub_key\"] == sub_key)\n",
        "        if len(rows) > mapped_count:\n",
        "            for row in rows[mapped_count:]:\n",
        "                unmapped.append({\n",
        "                    \"sub_key\": sub_key,\n",
        "                    \"csv_id\": row[\"id\"],\n",
        "                    \"csv_value\": row[\"value\"],\n",
        "                    \"reason\": \"extra_data_not_mapped\",\n",
        "                })\n",
        "\n",
        "    unmapped_df = pd.DataFrame(unmapped)\n",
        "\n",
        "    summary_rows = []\n",
        "    for sub_key in sorted(set(audit_df[\"sub_key\"])):\n",
        "        map_count = (audit_df[\"sub_key\"] == sub_key).sum()\n",
        "        data_count = len(data_rows.get(sub_key, []))\n",
        "        formula_count = ((audit_df[\"sub_key\"] == sub_key) & (audit_df[\"target_is_formula\"])).sum()\n",
        "        summary_rows.append({\n",
        "            \"sub_key\": sub_key,\n",
        "            \"mapping_cells\": map_count,\n",
        "            \"data_rows\": data_count,\n",
        "            \"formula_cells\": int(formula_count),\n",
        "            \"non_formula_cells\": int(map_count - formula_count),\n",
        "            \"status\": \"ok\" if map_count == data_count else \"count_mismatch\",\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "    audit_path = f\"{output_prefix}_mapping.csv\"\n",
        "    summary_path = f\"{output_prefix}_summary.csv\"\n",
        "    unmapped_path = f\"{output_prefix}_unmapped.csv\"\n",
        "\n",
        "    audit_df.to_csv(audit_path, index=False)\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    if not unmapped_df.empty:\n",
        "        unmapped_df.to_csv(unmapped_path, index=False)\n",
        "\n",
        "    return audit_df, summary_df, unmapped_df\n",
        "\n",
        "\n",
        "def rule_check_numeric_only(subset):\n",
        "    issues = []\n",
        "    values = pd.to_numeric(subset[\"value\"], errors=\"coerce\")\n",
        "    bad = subset[values.isna() & subset[\"value\"].notna()]\n",
        "    for row in bad.itertuples(index=False):\n",
        "        issues.append({\n",
        "            \"rule_id\": \"numeric_only\",\n",
        "            \"sub_key\": row.sub_key,\n",
        "            \"id\": row.id,\n",
        "            \"value\": row.value,\n",
        "            \"issue\": \"non_numeric_value\",\n",
        "            \"source\": \"data\",\n",
        "        })\n",
        "    return issues\n",
        "\n",
        "\n",
        "def rule_check_pages_require_requests(subset):\n",
        "    issues = []\n",
        "    target_subs = {\"2.4\", \"2.5\", \"4.5.2\", \"4.5.4\", \"4.5.6\", \"8.1\", \"8.2\"}\n",
        "\n",
        "    for sub_key in target_subs:\n",
        "        group = subset[subset[\"sub_key\"] == sub_key].copy()\n",
        "        if group.empty:\n",
        "            continue\n",
        "        comp = group[\"id\"].map(parse_id_components)\n",
        "        group = group[comp.notna()].copy()\n",
        "        comps = comp.dropna()\n",
        "        group[[\"row_num\", \"row_suffix\", \"cell_num\", \"cell_suffix\"]] = list(comps)\n",
        "        group[\"row_key\"] = group[\"row_num\"].astype(str) + group[\"row_suffix\"]\n",
        "        group[\"value_num\"] = pd.to_numeric(group[\"value\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "        for row_key, rows in group.groupby(\"row_key\"):\n",
        "            request_val = rows[rows[\"cell_num\"] == 1][\"value_num\"].sum()\n",
        "            pages_val = rows[rows[\"cell_num\"] > 1][\"value_num\"].sum()\n",
        "            if pages_val > 0 and request_val <= 0:\n",
        "                issues.append({\n",
        "                    \"rule_id\": \"pages_require_requests\",\n",
        "                    \"sub_key\": sub_key,\n",
        "                    \"row_key\": row_key,\n",
        "                    \"issue\": \"pages_minutes_without_requests\",\n",
        "                    \"detail\": f\"pages_or_minutes={pages_val}, requests={request_val}\",\n",
        "                    \"source\": \"data\",\n",
        "                })\n",
        "\n",
        "    return issues\n",
        "\n",
        "\n",
        "def rule_check_5_1_vs_4_1(subset):\n",
        "    issues = []\n",
        "    sub_41 = subset[subset[\"sub_key\"] == \"4.1\"].copy()\n",
        "    sub_51 = subset[subset[\"sub_key\"] == \"5.1\"].copy()\n",
        "\n",
        "    if sub_41.empty or sub_51.empty:\n",
        "        return issues\n",
        "\n",
        "    def build_row_totals(df):\n",
        "        comp = df[\"id\"].map(parse_id_components)\n",
        "        df = df[comp.notna()].copy()\n",
        "        comps = comp.dropna()\n",
        "        df[[\"row_num\", \"row_suffix\", \"cell_num\", \"cell_suffix\"]] = list(comps)\n",
        "        df[\"row_key\"] = df[\"row_num\"].astype(str) + df[\"row_suffix\"]\n",
        "        df[\"value_num\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\").fillna(0)\n",
        "        totals = df.groupby(\"row_key\")[\"value_num\"].sum().to_dict()\n",
        "        return totals\n",
        "\n",
        "    totals_41 = build_row_totals(sub_41)\n",
        "    totals_51 = build_row_totals(sub_51)\n",
        "\n",
        "    for row_key, total in totals_51.items():\n",
        "        if total > 0 and totals_41.get(row_key, 0) <= 0:\n",
        "            issues.append({\n",
        "                \"rule_id\": \"5_1_requires_4_1\",\n",
        "                \"sub_key\": \"5.1\",\n",
        "                \"row_key\": row_key,\n",
        "                \"issue\": \"section_5_1_disposition_without_4_1\",\n",
        "                \"detail\": f\"section_5_1_total={total}, section_4_1_total={totals_41.get(row_key, 0)}\",\n",
        "                \"source\": \"data\",\n",
        "            })\n",
        "\n",
        "    return issues\n",
        "\n",
        "\n",
        "def rule_check_section_1_totals(subset):\n",
        "    issues = []\n",
        "    sub = subset[subset[\"sub_key\"] == \"1.1\"].copy()\n",
        "    if sub.empty:\n",
        "        return issues\n",
        "\n",
        "    comp = sub[\"id\"].map(parse_id_components)\n",
        "    sub = sub[comp.notna()].copy()\n",
        "    comps = comp.dropna()\n",
        "    sub[[\"row_num\", \"row_suffix\", \"cell_num\", \"cell_suffix\"]] = list(comps)\n",
        "    sub = sub[sub[\"cell_num\"] == 1].copy()\n",
        "    sub[\"row_key\"] = sub[\"row_num\"].astype(str) + sub[\"row_suffix\"]\n",
        "    sub[\"value_num\"] = pd.to_numeric(sub[\"value\"], errors=\"coerce\").fillna(0)\n",
        "    values = sub.set_index(\"row_key\")[\"value_num\"].to_dict()\n",
        "\n",
        "    def check_eq(row_key, expected, label):\n",
        "        actual = values.get(row_key, 0)\n",
        "        if abs(actual - expected) > 1e-6:\n",
        "            issues.append({\n",
        "                \"rule_id\": \"section_1_1_total\",\n",
        "                \"sub_key\": \"1.1\",\n",
        "                \"row_key\": row_key,\n",
        "                \"issue\": label,\n",
        "                \"detail\": f\"expected={expected}, actual={actual}\",\n",
        "                \"source\": \"data\",\n",
        "            })\n",
        "\n",
        "    row2a = values.get(\"2a\", 0)\n",
        "    row2b = values.get(\"2b\", 0)\n",
        "    row2 = values.get(\"2\", 0)\n",
        "    row1 = values.get(\"1\", 0)\n",
        "    row5 = values.get(\"5\", 0)\n",
        "    row6 = values.get(\"6\", 0)\n",
        "    row7 = values.get(\"7\", 0)\n",
        "    row8 = values.get(\"8\", 0)\n",
        "    row9 = values.get(\"9\", 0)\n",
        "\n",
        "    check_eq(\"2\", row2a + row2b, \"row2_equals_row2a_plus_row2b\")\n",
        "    check_eq(\"5\", row1 + row2, \"row5_equals_row1_plus_row2\")\n",
        "    check_eq(\"7\", row8 + row9, \"row7_equals_row8_plus_row9\")\n",
        "    check_eq(\"7\", row5 - row6, \"row7_equals_row5_minus_row6\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "\n",
        "def rule_check_section_1_totals_generic(subset, sub_key):\n",
        "    issues = []\n",
        "    sub = subset[subset[\"sub_key\"] == sub_key].copy()\n",
        "    if sub.empty:\n",
        "        return issues\n",
        "\n",
        "    comp = sub[\"id\"].map(parse_id_components)\n",
        "    sub = sub[comp.notna()].copy()\n",
        "    comps = comp.dropna()\n",
        "    sub[[\"row_num\", \"row_suffix\", \"cell_num\", \"cell_suffix\"]] = list(comps)\n",
        "    sub = sub[sub[\"cell_num\"] == 1].copy()\n",
        "    sub[\"value_num\"] = pd.to_numeric(sub[\"value\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "    totals = sub.set_index(\"row_num\")[\"value_num\"].to_dict()\n",
        "    expected = sum(totals.get(i, 0) for i in range(1, 7))\n",
        "    actual = totals.get(7, 0)\n",
        "    if abs(actual - expected) > 1e-6:\n",
        "        issues.append({\n",
        "            \"rule_id\": \"section_total\",\n",
        "            \"sub_key\": sub_key,\n",
        "            \"row_key\": \"7\",\n",
        "            \"issue\": \"row7_total_mismatch\",\n",
        "            \"detail\": f\"expected={expected}, actual={actual}\",\n",
        "            \"source\": \"data\",\n",
        "        })\n",
        "\n",
        "    return issues\n",
        "\n",
        "\n",
        "def run_business_rules(subset):\n",
        "    issues = []\n",
        "    issues.extend(rule_check_numeric_only(subset))\n",
        "    issues.extend(rule_check_pages_require_requests(subset))\n",
        "    issues.extend(rule_check_5_1_vs_4_1(subset))\n",
        "    issues.extend(rule_check_section_1_totals(subset))\n",
        "    issues.extend(rule_check_section_1_totals_generic(subset, \"1.2\"))\n",
        "    issues.extend(rule_check_section_1_totals_generic(subset, \"1.3\"))\n",
        "    return pd.DataFrame(issues)\n",
        "\n",
        "\n",
        "def audit_all(df, gc_org_id, report_start, lang=\"En\", output_prefix=\"ati_audit\"):\n",
        "    report_start = pd.to_datetime(report_start, errors=\"coerce\").date()\n",
        "    subset = df[(df[\"gc_orgID\"] == gc_org_id) & (df[\"report_start\"] == report_start)]\n",
        "    if subset.empty:\n",
        "        raise ValueError(\"No rows found for the selected institution and period.\")\n",
        "\n",
        "    mapping_df, summary_df, unmapped_df = audit_mapping(\n",
        "        df,\n",
        "        gc_org_id=gc_org_id,\n",
        "        report_start=report_start,\n",
        "        lang=lang,\n",
        "        output_prefix=output_prefix,\n",
        "    )\n",
        "\n",
        "    rules_df = run_business_rules(subset)\n",
        "    rules_path = f\"{output_prefix}_rules.csv\"\n",
        "    rules_df.to_csv(rules_path, index=False)\n",
        "\n",
        "    return mapping_df, summary_df, unmapped_df, rules_df\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}